{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "os.cpu_count(): 20\n",
      "GPU 0: NVIDIA GeForce RTX 4070 SUPER (UUID: GPU-b10c6cac-b9b4-49fa-2e4d-8ba5c4c711ba)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from contextlib import nullcontext\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from cycler import cycler; plt.rcParams[\"axes.prop_cycle\"] = cycler(color=[\"#000000\", \"#2180FE\", \"#EB4275\"])\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'device: {device}')\n",
    "print(f'os.cpu_count(): {os.cpu_count()}')\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cube import Cube\n",
    "env = Cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding): LinearBlock(\n",
       "    (fc): Linear(in_features=324, out_features=5000, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (bn): BatchNorm1d(5000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (linear_layers): ModuleList(\n",
       "    (0): LinearBlock(\n",
       "      (fc): Linear(in_features=5000, out_features=1000, bias=True)\n",
       "      (relu): ReLU()\n",
       "      (bn): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (residual_blocks): ModuleList(\n",
       "    (0-3): 4 x ResidualBlock(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x LinearBlock(\n",
       "          (fc): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "          (relu): ReLU()\n",
       "          (bn): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=1000, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import Model\n",
    "model = Model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([1000, 26, 54])\n",
      "y shape: torch.Size([1000, 26])\n",
      "X dtype: torch.int64\n",
      "y dtype: torch.int64\n",
      "Sample X: tensor([[2, 0, 0,  ..., 5, 5, 5],\n",
      "        [2, 2, 2,  ..., 5, 5, 3],\n",
      "        [2, 2, 2,  ..., 1, 5, 3],\n",
      "        ...,\n",
      "        [1, 4, 2,  ..., 3, 2, 2],\n",
      "        [5, 4, 2,  ..., 2, 3, 3],\n",
      "        [5, 4, 2,  ..., 1, 3, 3]])\n",
      "Sample y: tensor([ 4,  0,  1, 11,  3,  8,  4,  6,  1, 11,  7, 11, 11,  3,  2,  5,  1,  8,\n",
      "         0, 10,  9,  7,  8,  5, 10,  1])\n",
      "Sample y: tensor([11,  7, 10,  3,  7,  3,  6,  1,  5,  0,  9, 10,  9,  8, 10,  0,  9,  9,\n",
      "         2,  1,  2,  7,  4,  4,  5,  0])\n"
     ]
    }
   ],
   "source": [
    "from config import TrainConfig\n",
    "\n",
    "\n",
    "class ScrambleGenerator(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_workers=os.cpu_count(),\n",
    "        max_depth=TrainConfig.max_depth,\n",
    "        total_samples=TrainConfig.num_steps*TrainConfig.batch_size_per_depth\n",
    "    ):\n",
    "        self.num_workers = num_workers\n",
    "        self.max_depth = max_depth\n",
    "        # self.envs = [Cube() for _ in range(num_workers)]\n",
    "        # self.generators = [env.scrambler(self.max_depth) for env in self.envs]\n",
    "\n",
    "        self.total_samples = total_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ''' generate one scramble, consisting of `self.max_depth` data points '''\n",
    "        # worker_idx = i % self.num_workers\n",
    "        try:\n",
    "            X = np.zeros((self.max_depth, 54), dtype=np.int64)\n",
    "            y = np.zeros((self.max_depth,), dtype=np.int64)\n",
    "            generator = Cube().scrambler(self.max_depth)\n",
    "            for j in range(self.max_depth):\n",
    "                state, last_move = next(generator)\n",
    "                X[j, :] = state\n",
    "                y[j] = last_move\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            print(f'error: {e}')\n",
    "            return self.__getitem__(i)\n",
    "\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    ScrambleGenerator(),\n",
    "    num_workers=0,\n",
    "    batch_size=TrainConfig.batch_size_per_depth\n",
    ")\n",
    "\n",
    "# Get one batch from the dataloader\n",
    "sample_batch = next(iter(dataloader))\n",
    "\n",
    "# Unpack X and y from the batch\n",
    "X, y = sample_batch\n",
    "\n",
    "# Check shapes and data types\n",
    "print(\"X shape:\", X.shape)  # Expected: (batch_size, max_depth, 54)\n",
    "print(\"y shape:\", y.shape)  # Expected: (batch_size, max_depth)\n",
    "print(\"X dtype:\", X.dtype)\n",
    "print(\"y dtype:\", y.dtype)\n",
    "\n",
    "# Optional: Print the first few entries to inspect contents\n",
    "print(\"Sample X:\", X[0])  # Print the first sample in X\n",
    "print(\"Sample y:\", y[0])  # Print the first sample in y\n",
    "print(\"Sample y:\", y[1])  # Print the first sample in y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training is disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Scarlet1ssimo\\AppData\\Local\\Temp\\ipykernel_50784\\515275775.py:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('10000steps.pth'))\n"
     ]
    }
   ],
   "source": [
    "def plot_loss_curve(h):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(h)\n",
    "    ax.set_xlabel(\"Steps\")\n",
    "    ax.set_ylabel(\"Cross-entropy loss\")\n",
    "    ax.set_xscale(\"log\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=TrainConfig.learning_rate)\n",
    "    g = iter(dataloader)\n",
    "    h = []\n",
    "    ctx = torch.cuda.amp.autocast(\n",
    "        dtype=torch.float16) if TrainConfig.ENABLE_FP16 else nullcontext()\n",
    "\n",
    "    for i in trange(1, TrainConfig.num_steps + 1):\n",
    "        batch_x, batch_y = next(g)\n",
    "        batch_x, batch_y = batch_x.reshape(-1,\n",
    "                                           54).to(device), batch_y.reshape(-1).to(device)\n",
    "\n",
    "        with ctx:\n",
    "            pred_y = model(batch_x)\n",
    "            loss = loss_fn(pred_y, batch_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        h.append(loss.item())\n",
    "        if TrainConfig.INTERVAL_PLOT and i % TrainConfig.INTERVAL_PLOT == 0:\n",
    "            clear_output()\n",
    "            plot_loss_curve(h)\n",
    "        if TrainConfig.INTERVAL_SAVE and i % TrainConfig.INTERVAL_SAVE == 0:\n",
    "            torch.save(model.state_dict(), f\"{i}steps.pth\")\n",
    "            print(\"Model saved.\")\n",
    "    print(\n",
    "        f\"Trained on data equivalent to {TrainConfig.batch_size_per_depth * TrainConfig.num_steps} solves.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "train_here = False\n",
    "if train_here:\n",
    "    model = train(model, dataloader)\n",
    "else:\n",
    "    print('training is disabled')\n",
    "    model.load_state_dict(torch.load('10000steps.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 42.31%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 42.31%\n",
      "Accuracy for subtask: 15 / 26 = 57.69%. Total accuracy: 47.44%\n",
      "Accuracy for subtask: 15 / 26 = 57.69%. Total accuracy: 50.00%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.23%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 47.44%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 46.70%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 46.63%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 47.01%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 46.54%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 47.20%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 46.79%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 47.04%\n",
      "Accuracy for subtask: 15 / 26 = 57.69%. Total accuracy: 47.80%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 47.44%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 47.84%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 47.51%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 47.44%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 47.57%\n",
      "Accuracy for subtask: 15 / 26 = 57.69%. Total accuracy: 48.08%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 48.17%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 47.73%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 47.66%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 47.28%\n",
      "Accuracy for subtask: 16 / 26 = 61.54%. Total accuracy: 47.85%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 47.63%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 47.72%\n",
      "Accuracy for subtask: 19 / 26 = 73.08%. Total accuracy: 48.63%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 48.81%\n",
      "Accuracy for subtask: 16 / 26 = 61.54%. Total accuracy: 49.23%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 49.38%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 49.04%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 48.95%\n",
      "Accuracy for subtask: 16 / 26 = 61.54%. Total accuracy: 49.32%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.23%\n",
      "Accuracy for subtask: 9 / 26 = 34.62%. Total accuracy: 48.82%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 48.96%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 48.68%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 48.62%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 48.65%\n",
      "Accuracy for subtask: 17 / 26 = 65.38%. Total accuracy: 49.06%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 49.18%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 49.28%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.30%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.32%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 49.08%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 48.85%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 48.80%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 48.82%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 48.62%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 48.72%\n",
      "Accuracy for subtask: 17 / 26 = 65.38%. Total accuracy: 49.04%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 49.13%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 49.22%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 49.09%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.04%\n",
      "Accuracy for subtask: 16 / 26 = 61.54%. Total accuracy: 49.26%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.27%\n",
      "Accuracy for subtask: 15 / 26 = 57.69%. Total accuracy: 49.41%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 49.29%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.24%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 49.32%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.27%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 49.16%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.11%\n",
      "Accuracy for subtask: 16 / 26 = 61.54%. Total accuracy: 49.30%\n",
      "Accuracy for subtask: 17 / 26 = 65.38%. Total accuracy: 49.54%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 49.43%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.39%\n",
      "Accuracy for subtask: 10 / 26 = 38.46%. Total accuracy: 49.23%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.19%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.20%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 49.10%\n",
      "Accuracy for subtask: 16 / 26 = 61.54%. Total accuracy: 49.27%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.28%\n",
      "Accuracy for subtask: 15 / 26 = 57.69%. Total accuracy: 49.39%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.35%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.36%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.37%\n",
      "Accuracy for subtask: 15 / 26 = 57.69%. Total accuracy: 49.47%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 49.53%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.48%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.44%\n",
      "Accuracy for subtask: 9 / 26 = 34.62%. Total accuracy: 49.27%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.28%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.28%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.29%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.26%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.27%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.23%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.20%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 49.12%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.09%\n",
      "Accuracy for subtask: 14 / 26 = 53.85%. Total accuracy: 49.14%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.15%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 49.08%\n",
      "Accuracy for subtask: 12 / 26 = 46.15%. Total accuracy: 49.05%\n",
      "Accuracy for subtask: 13 / 26 = 50.00%. Total accuracy: 49.06%\n",
      "Accuracy for subtask: 15 / 26 = 57.69%. Total accuracy: 49.15%\n",
      "Accuracy for subtask: 11 / 26 = 42.31%. Total accuracy: 49.08%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4907692307692308"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate Accuracy\n",
    "def validate(model, num_samples=100):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples):\n",
    "            env = Cube()\n",
    "            generator = env.scrambler(TrainConfig.max_depth)\n",
    "            sub_correct = 0\n",
    "            sub_cnt = 0\n",
    "            for i in range(TrainConfig.max_depth):\n",
    "                state, move = next(generator)\n",
    "                pred = model(torch.tensor(state, dtype=torch.int64).to(\n",
    "                    device)).argmax().item()\n",
    "                if pred == move:\n",
    "                    sub_correct += 1\n",
    "                sub_cnt+=1\n",
    "            correct += sub_correct\n",
    "            cnt += sub_cnt\n",
    "            print(f\"Accuracy for subtask: {sub_correct} / {sub_cnt} = {sub_correct/sub_cnt:.2%}. Total accuracy: {correct / cnt:.2%}\")\n",
    "    return correct / cnt\n",
    "\n",
    "\n",
    "validate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Scarlet1ssimo\\Documents\\444\\final\\DeepCubeA\n",
      "### Optimal Solver ###\n",
      "dict_keys(['states', 'times', 'solutions', 'num_nodes_generated'])\n",
      "No. of cases: 1000\n",
      "\n",
      "### DeepCubeA ###\n",
      "dict_keys(['states', 'solutions', 'paths', 'times', 'num_nodes_generated'])\n",
      "No. of cases: 1000\n",
      "c:\\Users\\Scarlet1ssimo\\Documents\\444\\final\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Scarlet1ssimo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "if \"DeepCubeA\"!=os.getcwd().split(\"/\")[-1]:\n",
    "    if not os.path.exists('DeepCubeA'):\n",
    "        !git clone -q https://github.com/forestagostinelli/DeepCubeA\n",
    "    %cd ./DeepCubeA/\n",
    "\n",
    "print('### Optimal Solver ###')\n",
    "filename = 'data/cube3/test/data_0.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    result_Optimal = pickle.load(f)\n",
    "\n",
    "    print(result_Optimal.keys())\n",
    "    result_Optimal[\"solution_lengths\"] = [len(s) for s in result_Optimal[\"solutions\"]]\n",
    "    result_Optimal[\"solution_lengths_count\"] = {\n",
    "        i: result_Optimal[\"solution_lengths\"].count(i)\n",
    "        for i in range(min(result_Optimal[\"solution_lengths\"]), max(result_Optimal[\"solution_lengths\"]))\n",
    "    }\n",
    "\n",
    "    print('No. of cases:', len(result_Optimal[\"solution_lengths\"]))\n",
    "\n",
    "print('\\n### DeepCubeA ###')\n",
    "filename = 'results/cube3/results.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    result_DeepCubeA = pickle.load(f)\n",
    "\n",
    "    print(result_DeepCubeA.keys())\n",
    "    result_DeepCubeA[\"solution_lengths\"] = [len(s) for s in result_DeepCubeA[\"solutions\"]]\n",
    "    result_DeepCubeA[\"solution_lengths_count\"] = {\n",
    "        i: result_DeepCubeA[\"solution_lengths\"].count(i)\n",
    "        for i in range(min(result_DeepCubeA[\"solution_lengths\"]), max(result_DeepCubeA[\"solution_lengths\"]))\n",
    "    }\n",
    "\n",
    "    print('No. of cases:', len(result_DeepCubeA[\"solution_lengths\"]))\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example:\n",
      "[['D', -1], ['F', 1], ['R', 1], ['U', -1], ['F', 1], ['F', 1], ['R', 1], ['U', 1], ['F', 1], ['R', 1], ['B', -1], ['R', -1], ['F', -1], ['R', -1], ['D', -1], ['U', -1], ['R', -1], ['U', -1], ['U', -1], ['R', -1], ['U', 1], ['B', -1]]\n",
      "-> ['B', \"U'\", 'R', 'U', 'U', 'R', 'U', 'D', 'R', 'F', 'R', 'B', \"R'\", \"F'\", \"U'\", \"R'\", \"F'\", \"F'\", 'U', \"R'\", \"F'\", 'D']\n"
     ]
    }
   ],
   "source": [
    "# Convert optimal solutions to test scrambles\n",
    "def solution2scramble(solution):\n",
    "    return [m[0] if m[1] == -1 else m[0] + \"'\" for m in solution[::-1]]\n",
    "\n",
    "test_scrambles = [solution2scramble(s) for s in result_Optimal[\"solutions\"]]\n",
    "\n",
    "print(f\"\"\"Example:\\n{result_Optimal[\"solutions\"][0]}\\n-> {test_scrambles[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import SearchConfig\n",
    "from cube import QTM\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search(\n",
    "    env,\n",
    "    model,\n",
    "    beam_width=SearchConfig.beam_width,\n",
    "    max_depth=SearchConfig.max_depth,\n",
    "    skip_redundant_moves=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Best-first search algorithm.\n",
    "    Input:\n",
    "        env: A scrambled instance of the given environment.\n",
    "        model: PyTorch model used to predict the next move(s).\n",
    "        beam_width: Number of top solutions to return per depth.\n",
    "        max_depth: Maximum depth of the search tree.\n",
    "        skip_redundant_moves: If True, skip redundant moves.\n",
    "    Output:\n",
    "        if solved successfully:\n",
    "            True, {'solutions':solution path, \"num_nodes_generated\":number of nodes expanded, \"times\":time taken to solve}\n",
    "        else:\n",
    "            False, None\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.cuda.amp.autocast(dtype=torch.float16) if SearchConfig.ENABLE_FP16 else nullcontext():\n",
    "        # metrics\n",
    "        num_nodes_generated, time_0 = 0, time.time()\n",
    "        candidates = [\n",
    "            {\"state\": deepcopy(env.state), \"path\": [], \"value\": 1.}\n",
    "        ]  # list of dictionaries\n",
    "\n",
    "        for depth in range(max_depth+1):\n",
    "            # TWO things at a time for every candidate: 1. check if solved & 2. add to batch_x\n",
    "            batch_x = np.zeros(\n",
    "                (len(candidates), env.state.shape[-1]), dtype=np.int64)\n",
    "            for i, c in enumerate(candidates):\n",
    "                c_path, env.state = c[\"path\"], c[\"state\"]\n",
    "                if c_path:\n",
    "                    env.move(c_path[-1])\n",
    "                    num_nodes_generated += 1\n",
    "                    if env.is_solved():\n",
    "                        # Revert: array of indices => array of notations\n",
    "                        c_path = [str(env.mode.NUM2CHAR[i]) for i in c_path]\n",
    "                        return True, {'solutions': c_path, \"num_nodes_generated\": num_nodes_generated, \"times\": time.time()-time_0}\n",
    "                batch_x[i, :] = env.state\n",
    "\n",
    "            # after checking the nodes expanded at the deepest\n",
    "            if depth == max_depth:\n",
    "                print(\"Solution not found.\")\n",
    "                return False, None\n",
    "\n",
    "            # make predictions with the trained DNN\n",
    "            batch_x = torch.from_numpy(batch_x).to(device)\n",
    "            batch_p = model(batch_x)\n",
    "            batch_p = torch.nn.functional.softmax(batch_p, dim=-1)\n",
    "            batch_p = batch_p.detach().cpu().numpy()\n",
    "\n",
    "            # loop over candidates\n",
    "            # storage for the depth-level candidates storing (path, value, index).\n",
    "            candidates_next_depth = []\n",
    "            for i, c in enumerate(candidates):\n",
    "                c_path = c[\"path\"]\n",
    "                # output logits for the given state\n",
    "                value_distribution = batch_p[i, :]\n",
    "                # multiply the cumulative probability so far of the expanded path\n",
    "                value_distribution *= c[\"value\"]\n",
    "\n",
    "                # iterate over all possible moves.\n",
    "                for m, value in enumerate(value_distribution):\n",
    "                    m = env.mode.getCancel(m)\n",
    "                    # predicted value to expand the path with the given move.\n",
    "\n",
    "                    if c_path and skip_redundant_moves:\n",
    "                        if m not in env.mode.MOVES_NO_CANCEL[c_path[-1]]:\n",
    "                            # Two mutually canceling moves\n",
    "                            continue\n",
    "                        elif len(c_path) > 1:\n",
    "                            # if c_path[-2] == c_path[-1] == m:\n",
    "                            if c_path[-2] == c_path[-1] == m:\n",
    "                                # Three subsequent moves that could be one\n",
    "                                continue\n",
    "                            # elif (\n",
    "                            #     c_path[-2][0] == m[0] and len(c_path[-2] + m) == 3\n",
    "                            #     and c_path[-1][0] == env.pairing[m[0]]\n",
    "                            # ):\n",
    "                            # elif env.mode.isOpposite(c_path[-2], c_path[-1]) and env.mode.isSameFace(j, c_path[-2]):\n",
    "                                # Two mutually canceling moves sandwiching an opposite face move\n",
    "                                # continue\n",
    "\n",
    "                    # add to the next-depth candidates unless 'continue'd.\n",
    "                    candidates_next_depth.append({\n",
    "                        'state': deepcopy(c['state']),\n",
    "                        \"path\": c_path+[m],\n",
    "                        \"value\": value,\n",
    "                    })\n",
    "\n",
    "            # sort potential paths by expected values and renew as 'candidates'\n",
    "            candidates = sorted(candidates_next_depth,\n",
    "                                key=lambda item: -item['value'])\n",
    "            # if the number of candidates exceed that of beam width 'beam_width'\n",
    "            candidates = candidates[:beam_width]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 38/1000 [01:22<35:00,  2.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m env\u001b[38;5;241m.\u001b[39mapply(scramble)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# solve\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m success, result \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m result_ours\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 44\u001b[0m, in \u001b[0;36mbeam_search\u001b[1;34m(env, model, beam_width, max_depth, skip_redundant_moves)\u001b[0m\n\u001b[0;32m     42\u001b[0m env\u001b[38;5;241m.\u001b[39mmove(c_path[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     43\u001b[0m num_nodes_generated \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_solved\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Revert: array of indices => array of notations\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     c_path \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mstr\u001b[39m(env\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mNUM2CHAR[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m c_path]\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolutions\u001b[39m\u001b[38;5;124m'\u001b[39m: c_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_nodes_generated\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_nodes_generated, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimes\u001b[39m\u001b[38;5;124m\"\u001b[39m: time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mtime_0}\n",
      "File \u001b[1;32mc:\\Users\\Scarlet1ssimo\\Documents\\444\\final\\cube.py:193\u001b[0m, in \u001b[0;36mCube.is_solved\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m54\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[1;32m--> 193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_solved\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m54\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m9\u001b[39m)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmove\u001b[39m(\u001b[38;5;28mself\u001b[39m, move):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result_ours = {\n",
    "    \"solutions\":[],\n",
    "    \"num_nodes_generated\":[],\n",
    "    \"times\":[]\n",
    "}\n",
    "for scramble in tqdm(test_scrambles, position=0):\n",
    "    # reset and scramble\n",
    "    env.reset()\n",
    "    env.apply(scramble)\n",
    "    # solve\n",
    "    success, result = beam_search(env, model)\n",
    "    if success:\n",
    "        for k in result_ours.keys():\n",
    "            result_ours[k].append(result[k])\n",
    "    else:\n",
    "        result_ours[\"solutions\"].append(None)\n",
    "\n",
    "result_ours['solution_lengths'] = [len(e) for e in result_ours['solutions'] if e]\n",
    "result_ours['solution_lengths_count'] = {\n",
    "    i: result_ours[\"solution_lengths\"].count(i)\n",
    "    for i in range(min(result_ours[\"solution_lengths\"]), max(result_ours[\"solution_lengths\"]))\n",
    "}\n",
    "f\"Successfully solved {len(result_ours['times'])} cases out of {len(result_ours['solutions'])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
